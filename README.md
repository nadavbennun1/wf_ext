# wf_ext

Simulation-based inference (SBI) has become a key approach for parameter estimation in complex evolutionary models where likelihood functions are intractable. In this work, I compared the performance of neural SBI methods to that of direct parameter regression approaches, specifically recurrent neural networks (RNNs) and state-space models (Mamba). All models were trained on simulations from the classic Wright-Fisher model and evaluated on both the original model and misspecified variants, including models with time-varying selection, population bottlenecks, and multiple interacting mutations. Neural SBI methods demonstrated greater robustness to model misspecification, maintaining accurate posterior approximations even when the data-generating process deviated substantially from the training model. In contrast, RNNs and Mamba achieved superior parameter recovery when the model assumptions were approximately correct, but their performance deteriorated under severe misspecifications. Additionally, predictive accuracy was found to be poorly correlated with true parameter recovery, suggesting that reliance on predictive checks alone may be misleading. These findings highlight important trade-offs between robustness and interpretability in neural inference pipelines for experimental evolution.