{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71fb4389-9bba-4f9b-bac7-a7dc168e9fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import the Mamba model\n",
    "from mamba_wf import Mamba, RMSNorm\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122c5963-4629-499f-a896-d5f2c32a32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([100000, 40, 1])\n",
      "Target data shape: torch.Size([100000, 7])\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "x = torch.load('test_sims/x_combined.pt')\n",
    "x = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "theta = torch.load('test_sims/theta_combined.pt')\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"Input data shape:\", x.shape)\n",
    "print(\"Target data shape:\", theta.shape)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 5e-4\n",
    "EPOCHS = 70\n",
    "TRAIN_SPLIT = 0.9  # 90% for training, 10% for validation\n",
    "\n",
    "# Extract dimensions from the data\n",
    "n_samples = x.shape[0]\n",
    "seq_len = x.shape[1]\n",
    "d_model = 16\n",
    "state_size = 64 \n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_size = int(n_samples * TRAIN_SPLIT)\n",
    "train_x = x[:train_size]\n",
    "train_theta = theta[:train_size]\n",
    "val_x = x[train_size:]\n",
    "val_theta = theta[train_size:]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(train_x, train_theta)\n",
    "val_dataset = TensorDataset(val_x, val_theta)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8df236d-b771-40bf-9e03-1ec41c7522ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add a linear output layer to map from d_model to n_params (which is theta.shape[1])\n",
    "class MambaWithOutputLayer(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, output_size, batch_size, device):\n",
    "        super(MambaWithOutputLayer, self).__init__()\n",
    "        self.mamba = Mamba(seq_len=seq_len, d_model=d_model, state_size=state_size, batch_size=batch_size, device=device)\n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # The Mamba output has shape [batch_size, seq_len, d_model]\n",
    "        # We need to process it to get [batch_size, output_size]\n",
    "        x = self.mamba(x)\n",
    "        # Take the last timestep output for simplicity\n",
    "        x = x[:, -1, :]  # Shape: [batch_size, d_model]\n",
    "        x = self.output_layer(x)  # Shape: [batch_size, output_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee125f9-8e43-4049-bc92-0f1e69ed305d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/70 - Training: 100%|██████████| 900/900 [00:59<00:00, 15.24it/s]\n",
      "Epoch 1/70 - Validation: 100%|██████████| 100/100 [00:00<00:00, 207.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70, Train Loss: 2.6843, Val Loss: 1.3122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/70 - Training: 100%|██████████| 900/900 [00:55<00:00, 16.09it/s]\n",
      "Epoch 2/70 - Validation: 100%|██████████| 100/100 [00:00<00:00, 208.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/70, Train Loss: 1.2594, Val Loss: 1.2469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/70 - Training:   2%|▏         | 18/900 [00:01<00:53, 16.39it/s]"
     ]
    }
   ],
   "source": [
    "# Create the model with output layer\n",
    "n_params = train_theta.shape[1]\n",
    "model = MambaWithOutputLayer(seq_len=seq_len, d_model=d_model, state_size=state_size, \n",
    "                            output_size=n_params, batch_size=BATCH_SIZE, device=device).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\"):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Validation\"):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': epoch_train_loss,\n",
    "                'val_loss': epoch_val_loss,\n",
    "            }, f'mamba_checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Plot training and validation loss\n",
    "def plot_loss(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Concatenate all outputs and targets\n",
    "    all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    return avg_val_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting training...\")\n",
    "    train_losses, val_losses = train()\n",
    "    plot_loss(train_losses, val_losses)\n",
    "    \n",
    "    print(\"\\nEvaluating model...\")\n",
    "    avg_val_loss = evaluate()\n",
    "    \n",
    "    print(\"Training and evaluation completed. Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d141008-51a5-4491-be84-f90d849ffb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:00<00:00, 237.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model on a test set from a specific simulator\n",
    "def test_mamba(sim):\n",
    "    # Load files and initialize datasets\n",
    "    test_x = torch.load(f'test_sims/test_x_{sim}.pt')\n",
    "    test_x = test_x.reshape(test_x.shape[0], test_x.shape[1], 1)\n",
    "    test_theta = torch.load(f'test_sims/test_theta_{sim}.pt')\n",
    "    test_dataset = TensorDataset(test_x, test_theta)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) # Can't shuffle if you want meaningful results :)\n",
    "\n",
    "    # Begin evaluating\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "            for inputs, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                predictions.append(outputs)\n",
    "\n",
    "    # Get all predictions\n",
    "    predictions = torch.cat(predictions)\n",
    "    torch.save(predictions, f'test_sims/predictions_{sim}_mamba_combined.pt')\n",
    "    \n",
    "    return predictions, test_theta\n",
    "\n",
    "for sim in ['combined']:\n",
    "    y = test_mamba(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8232ba1-aa64-4db6-8fe3-5a8219ed2749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44687"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count parameters in the Mamba model\n",
    "# Specifically here I show the 2-mutation, therefore numbers are different from Table 1\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2f94a-27e2-4102-bb44-bfe45638fe5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
